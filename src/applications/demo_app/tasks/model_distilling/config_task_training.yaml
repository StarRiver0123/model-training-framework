general:
  random_state: 42
  device:

net_structure:
  dataset:
    corpus_file: for_model_distilling/classification_data.csv
    train_set_file: for_model_distilling/test.csv
    use_data_augmentation: !!bool False
    train_augment_set_file: for_model_distilling/train_augment.csv
    valid_set_file: for_model_distilling/valid.csv
    test_set_file: for_model_distilling/test.csv
    valid_size: 0.01    # proportion of the sum of train set and valid set
    test_size: 0.01     # proportion of the sum of train set and valid set and test set
  training_whom: pure_student   # teacher or pure_student or distilled_student
  teacher_model: bert_c
  student_model: lstm_c
  trained_teacher_model_file: self_pretrained_models/model_epoch_000_loss_1.82380_f1_score_0.76973_20220402222158.pt
  pretrained_bert_model_file: macbert-base-chinese  # 如果对应模型的use_bert为False，则此设置失效
  use_pretrained_word_vector: !!bool True
  pretrained_word_vector_file: sgns.wiki.bigram-char        # 如果对应模型的use_bert为True，则此设置失效
  criterion: response_based_distil_loss   # ce, response_based_distil_loss
  optimizer: adamw
  lr_scheduler: cosdecay   # steplr or cosdecay, or self_adjusting
  evaluator: f1_score

train_text_transforming_adaptor:    # 如果数据样本的某个filed不需要进行转换处理比如标签label，则对应的transform_key付为空字符串或者None
  distilled_student:
    input_for_teacher: (0, 'input_for_bert')
    input_for_student: (0, 'input_for_nonbert')
    labels: (1, None)
  pure_student:
    input_for_student: (0, 'input_for_nonbert')
    labels: (1, None)
  teacher:
    input_for_teacher: (0, 'input_for_bert')
    labels: (1, None)

valid_text_transforming_adaptor:    # 如果数据样本的某个filed不需要进行转换处理比如标签label，则对应的transform_key付为空字符串或者None
  distilled_student:
    input_for_student: (0, 'input_for_nonbert')
    labels: (1, None)
  pure_student:
    input_for_student: (0, 'input_for_nonbert')
    labels: (1, None)
  teacher:
    input_for_teacher: (0, 'input_for_bert')
    labels: (1, None)

# 语料输入模型（转换成向量）之前的处理配置
text_transforming:    # 不仅限于对文本预料的处理，还可以对目标标签进行处理，比如NER任务的标签。
  input_for_bert:
    use_bert_style: !!bool True
    use_tokenizing: !!bool True             # 如果需要序列化，就要配置用什么tokenizer。注意即使模型使用bert，tokenizer也可以不用bert自带的，比如使用split
    tokenizer:                               # 如果use_tokenizing为False，则此设置失效    # bert, spacy, moses, toktok, revtok, subword, basic_english, 自定义的函数
    language:                                # 如果use_tokenizing为False，则此设置失效
    use_numericalizing: !!bool True          # 如果需要数字化，就需要对应的词典，这个词典要么从预料中统计，要么使用bert的词典
    corpus_field_index_for_vocab_building:   # 如果use_bert_style为True或use_numericalizing为False，则此设置失效    # 必须是单个，或者连续的多个（写成列表切片式）
    use_padding:                             # 如果use_bert_style为True，则此设置失效
    use_start_end_symbol:                    # 如果use_bert_style为True，则此设置失效
  input_for_nonbert:
    use_bert_style: !!bool False
    use_tokenizing: !!bool True             # 如果需要序列化，就要配置用什么tokenizer。注意即使模型使用bert，tokenizer也可以不用bert自带的，比如使用split
    tokenizer:                               # 如果use_tokenizing为False，则此设置失效    # bert, spacy, moses, toktok, revtok, subword, basic_english, 自定义的函数
    language:                                # 如果use_tokenizing为False，则此设置失效
    use_numericalizing: !!bool True          # 如果需要数字化，就需要对应的词典，这个词典要么从预料中统计，要么使用bert的词典
    corpus_field_index_for_vocab_building: 0 # 如果use_bert_style为True或use_numericalizing为False，则此设置失效    # 必须是单个，或者连续的多个（写成列表切片式）
    use_padding: !!bool True                 # 如果use_bert_style为True，则此设置失效
    use_start_end_symbol: !!bool True        # 如果use_bert_style为True，则此设置失效
    use_pretrained_word_vector: !!bool True
    pretrained_word_vector_file: sgns.wiki.bigram-char        # 如果对应模型的use_bert为True，则此设置失效
  labels_for_ner:
    use_bert_style: !!bool False
    use_tokenizing: !!bool False             # 如果需要序列化，就要配置用什么tokenizer。注意即使模型使用bert，tokenizer也可以不用bert自带的，比如使用split
    tokenizer:                               # 如果use_tokenizing为False，则此设置失效    # bert, spacy, moses, toktok, revtok, subword, basic_english, 自定义的函数
    language:                                # 如果use_tokenizing为False，则此设置失效
    use_numericalizing: !!bool True          # 如果需要数字化，就需要对应的词典，这个词典要么从预料中统计，要么使用bert的词典
    corpus_field_index_for_vocab_building: 1 # 如果use_bert_style为True或use_numericalizing为False，则此设置失效    # 必须是单个，或者连续的多个（写成列表切片式）
    use_padding: !!bool True                 # 如果use_bert_style为True，则此设置失效
    use_start_end_symbol: !!bool True        # 如果use_bert_style为True，则此设置失效

dataset:
  general_symbol:
    sos_token: <sos>
    eos_token: <eos>
    pad_token: <pad>
    unk_token: <unk>

model:
  bert_c:
    use_bert: !!bool True
    max_len: 48   # must be smaller than 512
    num_classes: 15
  lstm_c:
    use_bert: !!bool False
    d_model: 300
    hidden_size: 256
    num_layers: 3
    p_drop: 0.1
    max_len: 48   # must be smaller than 512
    num_classes: 15

criteria:
  lsce:
    label_smoothing: 0.1
  response_based_distil_loss:
    soft_loss_type: ce  # mse or ce
    T: 10
    alpha: 0.2

optimizer:
  adam:
    lr: !!float 1e-3
    beta1: 0.9
    beta2: 0.98
    eps: !!float 1e-9
  adamw:
    lr: !!float 2e-5      #翻译任务：1e-4，ner任务：3e-5 for bert, 1e-3 for lstm
    beta1: 0.9
    beta2: 0.999
    eps: !!float 1e-9

lr_scheduler:
  steplr:
    step_size: 100
    gamma: 0.999
  cosdecay:
    min_lr: !!float 1e-8
    step_size: 5          # 这个参数在函数内部被屏蔽了，因为存在一个隐藏的bug。详见函数注释。
    warmup_size: 0.02
  self_adjusting:
    mean_loss_window: 64
    cross_mean: !!bool False
    adjusting_ratio: 0.1
    warmup_size: 0.0
    smoothing_zero: !!float 1e-8

evaluator:

training:
  resume_from_check_point: !!bool False
  full_resume: !!bool False
  model_check_point_file:
  save_check_point: !!bool False
  check_point_epoch_step: 1
  epochs: 1
  batch_size: 32
  pretrained_bert_full_fine_tuning: !!bool True
  pretrained_word_vector_full_fine_tuning: !!bool True
  validating: !!bool True
  loss_threshold: 10.5
  evaluation_threshold: -0.6
  batch_interval_for_log: 10
  max_norm: 5

logging:
  sys_log_level: debug    # must be: debug, info, warning, error, critical
  file_log_level: debug
  console_log_level: info