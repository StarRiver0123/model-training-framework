general:
  random_state: 42
  device:

net_structure:
  dataset:
    corpus_tagging_file: 'for_ner/tagging.txt'
    train_tagging_file: 'for_ner/train_tagging.txt'
    test_tagging_file: 'for_ner/test_tagging.txt'
    gen_num_total_examples: 13418
    ner_parameter_file: 'for_ner/ner_parameter.pkl'
  use_bert: 'dynamic'  # null or none or static or dynamic
  bert_model_file: 'bert-distil-chinese'
  tokenizer:
  word_vector_file: 'sgns.wiki.bigram-char'
  model: 'bert_crf'    # bert_crf or lstm_crf
  criterion: 'crf'  # lsce or ce  or crf
  optimizer: 'adamw'
  lr_scheduler: 'cosdecay'   # steplr or warmup or cosdecay
  evaluator: 'f1_score'
  model_creator_module: 'build_model'   # 同目录下build_model.py的文件名，一般不要修改
  model_creator_class: 'NERModel'       # 同目录下build_model.py里的类名称

dataset:
  symbol:
    sos_token: '<sos>'
    eos_token: '<eos>'
    pad_token: '<pad>'
    unk_token: '<unk>'

model:
  starriver_transformer:
    num_encoder_layers : 6
    num_decoder_layers : 6
    d_model: 768 #300
    d_ff: 2048 #1024
    max_len: 128
    nhead: 12  #5
    p_drop: 0.1
  torch_transformer:
    num_encoder_layers : 6
    num_decoder_layers : 6
    d_model: 300 #768
    d_ff: 1024 #3072
    max_len: 128
    nhead: 5  #12
    p_drop: 0.1
  bert_crf:
    max_len: 64   # must be smaller than 512
    split_overlap_size: 8
  lstm_crf:
    d_model: 300
    hidden_size: 128
    num_layers: 3
    p_drop: 0.1
    max_len: 64   # must be smaller than 512
    split_overlap_size: 8
  twin_textrnn:
    d_model: 300
    hidden_size: 128
    num_layers: 3
    p_drop: 0.1
#    max_len: 512

criteria:
  lsce:
    label_smoothing: 0.1

optimizer:
  adam:
    lr: !!float 1e-3
    beta1: 0.9
    beta2: 0.98
    eps: !!float 1e-9
  adamw:
    lr: !!float 3e-5
    beta1: 0.9
    beta2: 0.999
    eps: !!float 1e-9

lr_scheduler:
  steplr:
    step_size: 100
    gamma: 0.999
  warmup:
    factor: 1
    step_size: 1
    lr_warmup_step: 40
  cosdecay:
    max_lr: !!float 3e-5      #翻译任务：1e-4，ner任务：3e-5 for bert, 1e-4 for lstm
    min_lr: !!float 1e-8
    step_size: 5          # 这个参数在函数内部被屏蔽了，因为存在一个隐藏的bug。详见函数注释。
    warmup_size: 0.02

evaluator:

training:
  resume_from_check_point: !!bool False
  model_check_point_file: 'check_point_epoch_004_loss_6.33063_20211218220318.pt'
  save_check_point: !!bool False
  check_point_epoch_step: 1
  epochs: 20
  batch_size: 4
  valid_size: 0.11    # proportion of the sum of train set and valid set
  test_size: 0.1     # proportion of the sum of train set and valid set and test set
  full_fine_tuning: !!bool True
  validating: !!bool True
  save_model: !!bool True
  model_save:
    save_mode: 'state'   #  'model' or 'state'
    save_model: !!bool True
    save_criterion: !!bool False
    save_optimizer: !!bool False
    save_lr_scheduler: !!bool False
    save_evaluator: !!bool False
  loss_threshold: 0.5
  evaluation_threshold: 0.7
  batch_interval_for_log: 10
  max_norm: 5

testing:
  saved_model_file: 'model_epoch_017_loss_0.00580_eval_0.89729_20220225014458-clue-data.pt'
  batch_size: 1    # must set to 1
  batch_interval_for_log: 1

logging:
  sys_log_level: 'debug'    # must be: debug, info, warning, error, critical
  file_log_level: 'debug'
  console_log_level: 'info'