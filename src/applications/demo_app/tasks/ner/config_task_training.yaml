general:
  random_state: 42
  device:

net_structure:
  dataset:
    dataset_class: mine   # clue or mine
    corpus_tagging_file: for_ner/tagging.txt
    train_set_file: for_ner/train.txt
    valid_set_file: for_ner/valid.txt
    test_set_file: for_ner/test.txt
#    use_enhancement: !!bool True
    gen_num_total_examples: 13418 #13418
    ner_parameter_file: for_ner/ner_parameter.pkl
    valid_size: 0.11    # proportion of the sum of train set and valid set
    test_size: 0.1     # proportion of the sum of train set and valid set and test set
  model: lstm_crf      # bert_crf or lstm_crf
  pretrained_bert_model_file: macbert-base-chinese  # 如果对应模型的use_bert为False，则此设置失效
  criterion: crf
  optimizer: adamw
  lr_scheduler: cosdecay   # steplr or cosdecay, or self_adjusting
  evaluator: f1_score


train_text_transforming_adaptor:    # 如果数据样本的某个filed不需要进行转换处理比如标签label，则对应的transform_key付为空字符串或者None
  bert_crf:
    input_seqs: (0, 'input_for_bert')
    ner_labels: (1, 'labels_for_ner')
  lstm_crf:
    input_seqs: (0, 'input_for_nonbert')
    ner_labels: (1, 'labels_for_ner')

valid_text_transforming_adaptor:    # 如果数据样本的某个filed不需要进行转换处理比如标签label，则对应的transform_key付为空字符串或者None
  bert_crf:
    input_seqs: (0, 'input_for_bert')
    ner_labels: (1, 'labels_for_ner')
  lstm_crf:
    input_seqs: (0, 'input_for_nonbert')
    ner_labels: (1, 'labels_for_ner')

# 语料输入模型（转换成向量）之前的处理配置
text_transforming:    # 不仅限于对文本预料的处理，还可以对目标标签进行处理，比如NER任务的标签。
  input_for_bert:
    use_bert_style: !!bool True
    use_tokenizing: !!bool True             # 如果需要序列化，就要配置用什么tokenizer。注意即使模型使用bert，tokenizer也可以不用bert自带的，比如使用split
    tokenizer:                               # 如果use_tokenizing为False，则此设置失效    # bert, spacy, moses, toktok, revtok, subword, basic_english, 自定义的函数
    language:                                # 如果use_tokenizing为False，则此设置失效
    use_numericalizing: !!bool True          # 如果需要数字化，就需要对应的词典，这个词典要么从预料中统计，要么使用bert的词典
    corpus_field_index_for_vocab_building:   # 如果use_bert_style为True或use_numericalizing为False，则此设置失效    # 必须是单个，或者连续的多个（写成列表切片式）
    use_padding:                             # 如果use_bert_style为True，则此设置失效
    use_start_end_symbol:                    # 如果use_bert_style为True，则此设置失效
  input_for_nonbert:
    use_bert_style: !!bool False
    use_tokenizing: !!bool True             # 如果需要序列化，就要配置用什么tokenizer。注意即使模型使用bert，tokenizer也可以不用bert自带的，比如使用split
    tokenizer:                               # 如果use_tokenizing为False，则此设置失效    # bert, spacy, moses, toktok, revtok, subword, basic_english, 自定义的函数
    language:                                # 如果use_tokenizing为False，则此设置失效
    use_numericalizing: !!bool True          # 如果需要数字化，就需要对应的词典，这个词典要么从预料中统计，要么使用bert的词典
    corpus_field_index_for_vocab_building: 0 # 如果use_bert_style为True或use_numericalizing为False，则此设置失效    # 必须是单个，或者连续的多个（写成列表切片式）
    use_padding: !!bool True                 # 如果use_bert_style为True，则此设置失效
    use_start_end_symbol: !!bool True        # 如果use_bert_style为True，则此设置失效
    use_pretrained_word_vector: !!bool True
    pretrained_word_vector_file: sgns.wiki.bigram-char        # 如果对应模型的use_bert为True，则此设置失效
  labels_for_ner:
    use_bert_style: !!bool False
    use_tokenizing: !!bool True             # 如果需要序列化，就要配置用什么tokenizer。注意即使模型使用bert，tokenizer也可以不用bert自带的，比如使用split
    tokenizer:                               # 如果use_tokenizing为False，则此设置失效    # bert, spacy, moses, toktok, revtok, subword, basic_english, 自定义的函数
    language:                                # 如果use_tokenizing为False，则此设置失效
    use_numericalizing: !!bool True          # 如果需要数字化，就需要对应的词典，这个词典要么从预料中统计，要么使用bert的词典
    corpus_field_index_for_vocab_building: 1 # 如果use_bert_style为True或use_numericalizing为False，则此设置失效    # 必须是单个，或者连续的多个（写成列表切片式）
    use_padding: !!bool True                 # 如果use_bert_style为True，则此设置失效
    use_start_end_symbol: !!bool True        # 如果use_bert_style为True，则此设置失效

dataset:
  general_symbol:
    sos_token: <sos>
    eos_token: <eos>
    pad_token: <pad>
    unk_token: <unk>

model:
  bert_crf:
    use_bert: !!bool True
    max_len: 128   # must be smaller than 512
    split_overlap_size: 8
  lstm_crf:
    use_bert: !!bool False
    d_model: 300
    hidden_size: 128
    num_layers: 3
    p_drop: 0.1
    max_len: 128   # must be smaller than 512
    split_overlap_size: 8

criteria:
  lsce:
    label_smoothing: 0.1

optimizer:
  adam:
    lr: !!float 1e-3
    beta1: 0.9
    beta2: 0.98
    eps: !!float 1e-9
  adamw:
    lr: !!float 2e-5      #翻译任务：1e-4，ner任务：3e-5 for bert, 1e-3 for lstm
    beta1: 0.9
    beta2: 0.999
    eps: !!float 1e-9

lr_scheduler:
  steplr:
    step_size: 100
    gamma: 0.999
  cosdecay:
    min_lr: !!float 1e-8
    step_size: 5          # 这个参数在函数内部被屏蔽了，因为存在一个隐藏的bug。详见函数注释。
    warmup_size: 0.02
  self_adjusting:
    mean_loss_window: 64
    cross_mean: !!bool False
    adjusting_ratio: 0.1
    warmup_size: 0.0
    smoothing_zero: !!float 1e-8

evaluator:

training:
  resume_from_check_point: !!bool False
  full_resume: !!bool False
  model_check_point_file:
  save_check_point: !!bool False
  check_point_epoch_step: 1
  epochs: 1
  batch_size: 8
  pretrained_bert_full_fine_tuning: !!bool True
  pretrained_word_vector_full_fine_tuning: !!bool True
  validating: !!bool True
  loss_threshold: 10.5
  evaluation_threshold: -10.6
  batch_interval_for_log: 10
  max_norm: 5

logging:
  sys_log_level: debug    # must be: debug, info, warning, error, critical
  file_log_level: debug
  console_log_level: info