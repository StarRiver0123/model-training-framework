general:
  random_state: 42
  device:

net_structure:
  dataset:
    train_json_file: 'for_translation/translation2019zh/translation2019zh_train.json'
    valid_json_file: 'for_translation/translation2019zh/translation2019zh_valid.json'
    src_corpus_file: 'for_translation/train.en'
    tgt_corpus_file: 'for_translation/train.zh'
    src_train_file: 'for_translation/use_static_bert/1000_en.txt'
    tgt_train_file: 'for_translation/use_static_bert/1000_zh.txt'
    src_test_file: 'for_translation/use_static_bert/1000_en.txt'
    tgt_test_file: 'for_translation/use_static_bert/1000_zh.txt'
  tokenizer:
    src_tokenizer: 'tokenize_en_byJieba'
    tgt_tokenizer: 'tokenize_zh_byJieba'
  word_vector:
    src_vectors_file: 'glove.6B.300d.txt'
    tgt_vectors_file: 'sgns.wiki.bigram-char'
  model: 'torch_transformer'  # starriver_transformer or torch_transformer
  criterion: 'ce'  # lsce or ce
  optimizer: 'adam'
  lr_scheduler: 'cosdecay'   # steplr or warmup or cosdecay
  evaluator: 'bleu'

dataset:
  general_symbol:
    sos_token: '<sos>'
    eos_token: '<eos>'
    pad_token: '<pad>'
    unk_token: '<unk>'

model:
  starriver_transformer:
    use_bert: !!bool False
    num_encoder_layers : 6
    num_decoder_layers : 6
    d_model: 300 #768 #300
    d_ff: 2048
    max_len: 128
    nhead: 5  #12  #5
    p_drop: 0.1
  torch_transformer:
    use_bert: !!bool False
    num_encoder_layers : 6
    num_decoder_layers : 6
    d_model: 300 #768
    d_ff: 1024 #3072
    max_len: 128
    nhead: 5  #12
    p_drop: 0.1

criteria:
  lsce:
    label_smoothing: 0.1

optimizer:
  adam:
    lr: !!float 1e-3
    beta1: 0.9
    beta2: 0.98
    eps: !!float 1e-9
  adamw:
    lr: !!float 3e-5
    beta1: 0.9
    beta2: 0.999
    eps: !!float 1e-9

lr_scheduler:
  steplr:
    step_size: 100
    gamma: 0.999
  warmup:
    factor: 1
    step_size: 1
    lr_warmup_step: 40
  cosdecay:
    max_lr: !!float 1e-4      #翻译任务：1e-4，ner任务：3e-5 for bert, 1e-4 for lstm
    min_lr: !!float 1e-8
    step_size: 5          # 这个参数在函数内部被屏蔽了，因为存在一个隐藏的bug。详见函数注释。
    warmup_size: 0.02

evaluator:

training:
  resume_from_check_point: !!bool False
  model_check_point_file: '.pt'
  save_check_point: !!bool False
  check_point_epoch_step: 1
  epochs: 1
  batch_size: 4
  valid_size: 0.002    # proportion of the sum of train set and valid set
  test_size: 0.002     # proportion of the sum of train set and valid set and test set
  bert_full_fine_tuning: !!bool True
  validating: !!bool True
  loss_threshold: 10  #2
  evaluation_threshold: -10 # 0.1
  batch_interval_for_log: 100
  max_norm: 5

logging:
  sys_log_level: 'debug'    # must be: debug, info, warning, error, critical
  file_log_level: 'debug'
  console_log_level: 'info'