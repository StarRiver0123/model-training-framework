general:
  random_state: 42
  device:

net_structure:
  trans_direct: 'en2zh'     # 'en2zh' or 'zh2en'
  dataset:
    train_json_file: 'for_translation/translation2019zh/translation2019zh_train.json'
    valid_json_file: 'for_translation/translation2019zh/translation2019zh_valid.json'
    corpus_en_file: 'for_translation/train.en'
    corpus_zh_file: 'for_translation/train.zh'
    train_en_file: 'for_translation/use_static_bert/1000_en.txt'
    train_zh_file: 'for_translation/use_static_bert/1000_zh.txt'
    test_en_file: 'for_translation/use_static_bert/test_en.txt'
    test_zh_file: 'for_translation/use_static_bert/test_zh.txt'
  use_bert: ''  # null or none or static or dynamic
  bert_model:
    bert_model_zh_file: 'bert-wwm-ext-chinese'
    bert_model_en_file: 'bert-base-uncased'
  tokenizer:
    tokenizer_en: 'tokenize_en_byJieba'
    tokenizer_zh: 'tokenize_zh_byJieba'
  word_vector:
    word_vectors_en_file: 'glove.6B.300d.txt'
    word_vectors_zh_file: 'sgns.wiki.bigram-char'
  model: 'starriver_transformer'  # starriver_transformer or torch_transformer
  criterion: 'ce'  # lsce or ce
  optimizer: 'adam'
  lr_scheduler: 'cosdecay'   # steplr or warmup or cosdecay
  evaluator: 'bleu'
  model_creator_module: 'build_model'       # 同目录下build_model.py的文件名，一般不要修改
  model_creator_class: 'TranslatorModel'    # 同目录下build_model.py里的类名称

dataset:
  symbol:
    sos_token: '<sos>'
    eos_token: '<eos>'
    pad_token: '<pad>'
    unk_token: '<unk>'

model:
  starriver_transformer:
    num_encoder_layers : 6
    num_decoder_layers : 6
    d_model: 300 #768 #300
    d_ff: 2048
    max_len: 128
    nhead: 5  #12  #5
    p_drop: 0.1
  torch_transformer:
    num_encoder_layers : 6
    num_decoder_layers : 6
    d_model: 300 #768
    d_ff: 1024 #3072
    max_len: 128
    nhead: 5  #12
    p_drop: 0.1

criteria:
  lsce:
    label_smoothing: 0.1

optimizer:
  adam:
    lr: !!float 1e-3
    beta1: 0.9
    beta2: 0.98
    eps: !!float 1e-9
  adamw:
    lr: !!float 3e-5
    beta1: 0.9
    beta2: 0.999
    eps: !!float 1e-9

lr_scheduler:
  steplr:
    step_size: 100
    gamma: 0.999
  warmup:
    factor: 1
    step_size: 1
    lr_warmup_step: 40
  cosdecay:
    max_lr: !!float 1e-4      #翻译任务：1e-4，ner任务：3e-5 for bert, 1e-4 for lstm
    min_lr: !!float 1e-8
    step_size: 5          # 这个参数在函数内部被屏蔽了，因为存在一个隐藏的bug。详见函数注释。
    warmup_size: 0.02

evaluator:

training:
  resume_from_check_point: !!bool False
  model_check_point_file: '.pt'
  save_check_point: !!bool True
  check_point_epoch_step: 1
  epochs: 50
  batch_size: 4
  valid_size: 0.002    # proportion of the sum of train set and valid set
  test_size: 0.002     # proportion of the sum of train set and valid set and test set
  bert_full_fine_tuning: !!bool True
  validating: !!bool True
  save_model: !!bool True
  model_save:
    save_mode: 'state'   #  'model' or 'state'
    save_model: !!bool True
    save_criterion: !!bool False
    save_optimizer: !!bool False
    save_lr_scheduler: !!bool False
    save_evaluator: !!bool False
  loss_threshold: 2
  evaluation_threshold: 0.1
  batch_interval_for_log: 100
  max_norm: 5

testing:
  saved_model_file: '.pt'
  batch_size: 1    # must set to 1
  batch_interval_for_log: 1

logging:
  sys_log_level: 'debug'    # must be: debug, info, warning, error, critical
  file_log_level: 'debug'
  console_log_level: 'info'